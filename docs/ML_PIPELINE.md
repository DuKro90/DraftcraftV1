# ğŸ§  Machine Learning Pipeline

**DraftcraftV1 - ML Workflow & Model Details**
**Version:** 2.3.0 | **Stand:** Dezember 2024

---

## ğŸ¯ ML-System Ãœbersicht

Das System nutzt **3 ML-Komponenten** in einer sequenziellen Pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML PIPELINE ARCHITECTURE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   INPUT: PDF/Image (German Document)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STAGE 1: OCR (Computer Vision)                 â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
    â”‚  Model: PaddleOCR 2.7 (German)                  â”‚
    â”‚  Task:  Image â†’ Text + Bounding Boxes           â”‚
    â”‚  Output: Raw text with confidence scores        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STAGE 2: NER (Natural Language Processing)     â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
    â”‚  Model: spaCy 3.8 de_core_news_lg + Custom      â”‚
    â”‚  Task:  Text â†’ Structured Entities              â”‚
    â”‚  Output: Entities (Material, Dimension, etc.)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  STAGE 3: LLM Enhancement (Optional)            â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
    â”‚  Model: Google Gemini 1.5 Flash                 â”‚
    â”‚  Task:  Verify/Enhance low-confidence results   â”‚
    â”‚  Output: Enhanced entities + explanations       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   OUTPUT: Structured Business Data      â”‚
         â”‚   (Ready for Calculation Engine)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ STAGE 1: OCR Pipeline (PaddleOCR)

### Model Details

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PADDLEOCR CONFIGURATION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model:        PaddleOCR 2.7.0.3                            â”‚
â”‚  Language:     German ('de')                                â”‚
â”‚  Architecture: PP-OCRv3 (Detection + Recognition)           â”‚
â”‚  Weights:      Pre-trained on German text corpus            â”‚
â”‚  Input Size:   Dynamic (auto-scaled to 960px width)         â”‚
â”‚  GPU Support:  Optional (CPU fallback enabled)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Two-Stage OCR Process

```
INPUT: PDF Page / Image
  â”‚
  â”œâ”€â–º STEP 1: TEXT DETECTION (Bounding Box Prediction)
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Model: DB (Differentiable Binarization)            â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
  â”‚   â”‚  Input:   Image (HÃ—WÃ—3, RGB)                        â”‚
  â”‚   â”‚  Process: Convolutional Neural Network              â”‚
  â”‚   â”‚  Output:  Bounding Boxes [(x1,y1), (x2,y2), ...]    â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Algorithm Flow:                                     â”‚
  â”‚   â”‚  1. Image Resizing: 960px width (preserve aspect)   â”‚
  â”‚   â”‚  2. CNN Feature Extraction: ResNet-18 backbone       â”‚
  â”‚   â”‚  3. Binary Segmentation: Text vs Background         â”‚
  â”‚   â”‚  4. Contour Detection: Polygon approximation        â”‚
  â”‚   â”‚  5. Box Filtering: Min area 10pxÂ², aspect ratio     â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Performance:                                        â”‚
  â”‚   â”‚  - Detection Rate: ~98% for printed text            â”‚
  â”‚   â”‚  - Speed: ~0.5s per page (CPU)                      â”‚
  â”‚   â”‚  - False Positives: <2% (logos, decorations)        â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚   Output: List of text regions with coordinates
  â”‚
  â”œâ”€â–º STEP 2: TEXT RECOGNITION (Character-Level OCR)
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Model: CRNN (Convolutional RNN)                    â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
  â”‚   â”‚  Input:   Cropped text regions (grayscale)          â”‚
  â”‚   â”‚  Process: CNN (features) + RNN (sequence)           â”‚
  â”‚   â”‚  Output:  Text string + confidence per character    â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Algorithm Flow:                                     â”‚
  â”‚   â”‚  FOR each bounding box:                              â”‚
  â”‚   â”‚    1. Crop & Normalize: 32px height, grayscale      â”‚
  â”‚   â”‚    2. CNN Encoding: VGG-like feature extraction     â”‚
  â”‚   â”‚    3. RNN Decoding: LSTM layers (bidirectional)     â”‚
  â”‚   â”‚    4. CTC Decoding: Sequence-to-text alignment      â”‚
  â”‚   â”‚    5. Confidence: Softmax probabilities averaged    â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  German-Specific Optimizations:                     â”‚
  â”‚   â”‚  âœ“ Umlauts (Ã¤, Ã¶, Ã¼, ÃŸ) in training data            â”‚
  â”‚   â”‚  âœ“ Number format: 1.234,56 (dot thousands)          â”‚
  â”‚   â”‚  âœ“ Special chars: â‚¬, Â°, Â² (common in construction)  â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Performance:                                        â”‚
  â”‚   â”‚  - Character Accuracy: 96% (printed), 75% (hand)    â”‚
  â”‚   â”‚  - Speed: ~1.5s per page (CPU)                      â”‚
  â”‚   â”‚  - Confidence: 0.0-1.0 per word                     â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚   Output: Full text with word-level confidence scores
  â”‚
  â””â”€â–º AGGREGATION: Combine results
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Combine Detection + Recognition                    â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
      â”‚  - Merge bounding boxes + text strings              â”‚
      â”‚  - Calculate overall page confidence                â”‚
      â”‚  - Spatial ordering (top-to-bottom, left-to-right)  â”‚
      â”‚  - Handle multi-column layouts                      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Output: OCRResult(text, confidence, positions)
```

### Preprocessing Pipeline

```
Raw Image
  â”‚
  â”œâ”€â–º [1] FORMAT DETECTION
  â”‚   â””â”€â–º PDF â†’ Convert to images (pdf2image, DPI=300)
  â”‚
  â”œâ”€â–º [2] IMAGE QUALITY ASSESSMENT
  â”‚   â”œâ”€ Check resolution (min 200 DPI)
  â”‚   â”œâ”€ Brightness histogram analysis
  â”‚   â””â”€ Blur detection (Laplacian variance)
  â”‚   â†“ Low quality? â†’ Apply preprocessing
  â”‚
  â”œâ”€â–º [3] PREPROCESSING (Conditional)
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  IF brightness < 100 OR > 200:                  â”‚
  â”‚   â”‚    â†’ Histogram equalization                     â”‚
  â”‚   â”‚                                                  â”‚
  â”‚   â”‚  IF blur_score < 100:                           â”‚
  â”‚   â”‚    â†’ Sharpening (Unsharp Mask)                  â”‚
  â”‚   â”‚                                                  â”‚
  â”‚   â”‚  IF noise detected:                             â”‚
  â”‚   â”‚    â†’ Gaussian blur (Ïƒ=0.5)                      â”‚
  â”‚   â”‚                                                  â”‚
  â”‚   â”‚  IF rotation != 0Â°:                             â”‚
  â”‚   â”‚    â†’ Deskew (Hough transform)                   â”‚
  â”‚   â”‚                                                  â”‚
  â”‚   â”‚  Special: Handwriting detection                 â”‚
  â”‚   â”‚    â†’ Increase contrast (+150%)                  â”‚
  â”‚   â”‚    â†’ Binarization (Otsu threshold)              â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â””â”€â–º [4] FORMAT NORMALIZATION
      â”œâ”€ Resize to 960px width (preserve aspect)
      â”œâ”€ Convert to RGB (if grayscale)
      â””â”€ Padding to 32px multiple (model requirement)
      â†“
      Ready for PaddleOCR
```

### Output Structure

```python
# ocr_service.py returns:
@dataclass
class OCRResult:
    full_text: str              # Complete extracted text
    confidence: float           # Average confidence (0.0-1.0)
    pages: List[PageResult]     # Per-page results
    processing_time: float      # Seconds

@dataclass
class PageResult:
    page_number: int
    text_boxes: List[TextBox]
    page_confidence: float

@dataclass
class TextBox:
    text: str                   # Extracted text snippet
    confidence: float           # Box-level confidence
    bbox: List[Tuple[int, int]] # [(x1,y1), (x2,y2), (x3,y3), (x4,y4)]
    position: str               # "header" | "body" | "footer"
```

### Error Handling

```
OCR Failure Scenarios:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. EMPTY RESULT (confidence = 0.0)
   Causes: Blank page, image-only PDF
   Action: Skip page, log warning

2. LOW CONFIDENCE (< 0.70)
   Causes: Bad scan quality, handwriting, Fraktur script
   Action: Route to AGENT_EXTRACT (Gemini)

3. ENCODING ERROR
   Causes: Special characters, non-German text
   Action: Fallback encoding (UTF-8 â†’ Windows-1252)

4. TIMEOUT (> 10s per page)
   Causes: Large image, CPU overload
   Action: Reduce resolution, retry with DPI=150

5. PADDLEOCR CRASH
   Causes: Out of memory, corrupted model
   Action: Service restart, alert admin
```

---

## ğŸ”¤ STAGE 2: NER Pipeline (spaCy)

### Model Details

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SPACY CONFIGURATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model:         de_core_news_lg (Large German)              â”‚
â”‚  Version:       spaCy 3.8.0                                 â”‚
â”‚  Training Data: German news + custom Handwerk corpus        â”‚
â”‚  Vocabulary:    500,000+ tokens                             â”‚
â”‚  Embeddings:    Word2Vec 300-dimensional                    â”‚
â”‚  Pipeline:      tok2vec â†’ tagger â†’ parser â†’ ner             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### NER Pipeline Architecture

```
INPUT: OCR Text (string)
  â”‚
  â”œâ”€â–º STEP 1: TOKENIZATION
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Component: Tokenizer (German rules)                â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
  â”‚   â”‚  Rules:                                              â”‚
  â”‚   â”‚  - Split on whitespace + punctuation                â”‚
  â”‚   â”‚  - Keep German compounds (e.g., "Eichen-Schreibtisch") â”‚
  â”‚   â”‚  - Preserve units (e.g., "25 mÂ²" â†’ ["25", "mÂ²"])    â”‚
  â”‚   â”‚  - Handle currency (e.g., "1.234,56 â‚¬")             â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Example:                                            â”‚
  â”‚   â”‚  Input:  "Tisch aus Eiche, 1,5 mÂ², geÃ¶lt"           â”‚
  â”‚   â”‚  Output: ["Tisch", "aus", "Eiche", ",",             â”‚
  â”‚   â”‚           "1,5", "mÂ²", ",", "geÃ¶lt"]                â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º STEP 2: PART-OF-SPEECH TAGGING
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Component: Tagger (Morphology)                     â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
  â”‚   â”‚  Tags: NOUN, VERB, ADJ, NUM, etc. (German tagset)   â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Example:                                            â”‚
  â”‚   â”‚  "Eiche" â†’ NOUN (Nominativ, Feminin, Singular)      â”‚
  â”‚   â”‚  "geÃ¶lt" â†’ ADJ (past participle)                    â”‚
  â”‚   â”‚  "1,5"   â†’ NUM (cardinal)                           â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º STEP 3: DEPENDENCY PARSING
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Component: Parser (Syntax tree)                    â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
  â”‚   â”‚  Builds grammatical relationships:                  â”‚
  â”‚   â”‚  - Subject â†’ Verb â†’ Object                          â”‚
  â”‚   â”‚  - Adjective â†’ Noun                                 â”‚
  â”‚   â”‚  - Number â†’ Unit                                    â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Example Tree:                                       â”‚
  â”‚   â”‚  Tisch â† ROOT                                       â”‚
  â”‚   â”‚    â”œâ”€â”€ aus â† prep                                   â”‚
  â”‚   â”‚    â”‚   â””â”€â”€ Eiche â† pobj (Material!)                 â”‚
  â”‚   â”‚    â””â”€â”€ geÃ¶lt â† amod (Surface!)                      â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º STEP 4: NAMED ENTITY RECOGNITION
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Component: NER (Custom labels)                     â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
  â”‚   â”‚  Base Model: Transformer-based (BERT-like)          â”‚
  â”‚   â”‚  Training: Pre-trained + fine-tuned                 â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Entity Labels (Custom for Handwerk):               â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
  â”‚   â”‚  MATERIAL      - Holzarten, Metalle, Stoffe         â”‚
  â”‚   â”‚  DIMENSION     - MaÃŸe (mÂ², lfm, Stk, kg)            â”‚
  â”‚   â”‚  MONEY         - Preise (â‚¬, USD)                    â”‚
  â”‚   â”‚  DATE          - Termine (DD.MM.YYYY)               â”‚
  â”‚   â”‚  QUANTITY      - Mengen (StÃ¼ckzahl)                 â”‚
  â”‚   â”‚  SURFACE       - OberflÃ¤chenbehandlung              â”‚
  â”‚   â”‚  COMPLEXITY    - Verarbeitungs-Indikatoren          â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Recognition Algorithm:                             â”‚
  â”‚   â”‚  1. Token embeddings (tok2vec)                      â”‚
  â”‚   â”‚  2. BiLSTM encoding (context)                       â”‚
  â”‚   â”‚  3. CRF decoding (BIO tagging)                      â”‚
  â”‚   â”‚     B-MATERIAL (Begin), I-MATERIAL (Inside)         â”‚
  â”‚   â”‚  4. Confidence: Softmax probabilities               â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Example Output:                                     â”‚
  â”‚   â”‚  "Eiche"    â†’ MATERIAL (conf: 0.94)                 â”‚
  â”‚   â”‚  "1,5 mÂ²"   â†’ DIMENSION (conf: 0.98)                â”‚
  â”‚   â”‚  "geÃ¶lt"    â†’ SURFACE (conf: 0.89)                  â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â–º STEP 5: RELATION EXTRACTION
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚  Custom Logic (Rule-based + ML)                     â”‚
  â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
  â”‚   â”‚  Links entities via dependency tree:                â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  IF MATERIAL â† prep â†’ QUANTITY:                     â”‚
  â”‚   â”‚    â†’ Link: "Eiche" + "1,5 mÂ²"                       â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  IF SURFACE â† amod â†’ MATERIAL:                      â”‚
  â”‚   â”‚    â†’ Link: "geÃ¶lt" applies to "Eiche"               â”‚
  â”‚   â”‚                                                      â”‚
  â”‚   â”‚  Output: Entity Graph                               â”‚
  â”‚   â”‚  {                                                   â”‚
  â”‚   â”‚    "material": "Eiche",                             â”‚
  â”‚   â”‚    "dimension": "1,5 mÂ²",                           â”‚
  â”‚   â”‚    "surface": "geÃ¶lt"                               â”‚
  â”‚   â”‚  }                                                   â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â””â”€â–º STEP 6: POST-PROCESSING
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Normalization & Validation                         â”‚
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
      â”‚  - Number parsing: "1,5" â†’ Decimal(1.5)             â”‚
      â”‚  - Unit standardization: "qm" â†’ "mÂ²"                â”‚
      â”‚  - Material lookup: "Eiche" â†’ MaterialList check    â”‚
      â”‚  - Confidence aggregation: avg(entity_scores)       â”‚
      â”‚  - Duplicate removal                                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Output: List[Entity] + overall_confidence
```

### Custom Training Data

```
Training Corpus for German Handwerk:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Source 1: Annotated Real Documents (500+ samples)
  â”œâ”€ Collected from beta testers
  â”œâ”€ Manually labeled entities
  â””â”€ Example annotations in IOB format:
      "Tisch O
       aus   O
       Eiche B-MATERIAL
       ,     O
       1,5   B-DIMENSION
       mÂ²    I-DIMENSION"

Source 2: Synthetic Data Generation (10,000+ samples)
  â”œâ”€ Templates: "{MATERIAL} Tisch, {DIMENSION}, {SURFACE}"
  â”œâ”€ Variables from TIER 1 data
  â””â”€ Augmentation: Synonyms, word order variations

Fine-Tuning Process:
  1. Load de_core_news_lg (base model)
  2. Add custom entity labels
  3. Train NER component (10 epochs)
  4. Validation split: 80% train / 20% test
  5. Metrics: F1 score > 0.90 (target)
```

### Performance Metrics

```
NER Evaluation (Test Set: 100 documents):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Entity Type      Precision  Recall  F1-Score  Samples
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MATERIAL         0.96       0.93    0.94      150
DIMENSION        0.98       0.97    0.97      200
MONEY            0.99       0.98    0.98      180
SURFACE          0.91       0.87    0.89      80
COMPLEXITY       0.88       0.83    0.85      60
DATE             0.95       0.94    0.94      120
QUANTITY         0.97       0.96    0.96      140
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OVERALL          0.95       0.93    0.94      930
```

---

## ğŸ¤– STAGE 3: LLM Enhancement (Gemini)

### When LLM is Used

```
Confidence-Based Routing:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Confidence Score    Action              LLM Task
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â‰¥ 0.92              AUTO_ACCEPT         None (skip LLM)
0.80 - 0.92         AGENT_VERIFY        Verify entities
0.70 - 0.80         AGENT_EXTRACT       Re-extract entities
< 0.70              HUMAN_REVIEW        None (manual)
```

### Model Configuration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GEMINI CONFIGURATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model:           gemini-1.5-flash                          â”‚
â”‚  Version:         Latest (API auto-updates)                 â”‚
â”‚  Context Window:  1M tokens (massive)                       â”‚
â”‚  Temperature:     0.1 (deterministic, low creativity)       â”‚
â”‚  Top-P:           0.95                                      â”‚
â”‚  Max Tokens:      2,048 (typical response: 200-500)         â”‚
â”‚  Safety Settings: BLOCK_MEDIUM_AND_ABOVE (German laws)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt Engineering

```
System Prompt Template:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Du bist ein Experte fÃ¼r deutsches Handwerk, spezialisiert auf
Schreiner-, Zimmerer- und Polsterer-Arbeiten.

Deine Aufgabe: {TASK}

KONTEXT:
- Kunde: {CUSTOMER_NAME} (Historie: {PREVIOUS_PROJECTS})
- Aktuelles Projekt: {PROJECT_TYPE}
- Bisherige Extraktion (OCR+NER):
  {EXTRACTED_ENTITIES}

TIER-DATEN (VerfÃ¼gbare Materialien/OberflÃ¤chen):
{TIER1_DATA}

BEISPIELE (Ã„hnliche Projekte):
{MEMORY_CONTEXT}

AUFGABE:
{SPECIFIC_INSTRUCTION}

AUSGABE-FORMAT (JSON):
{
  "entities": [
    {"type": "MATERIAL", "value": "...", "confidence": 0.0-1.0},
    ...
  ],
  "explanation": "BegrÃ¼ndung fÃ¼r Entscheidung",
  "changes": ["Was wurde korrigiert"]
}
```

### Task-Specific Prompts

```
AGENT_VERIFY (Confidence 0.80-0.92):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AUFGABE:
ÃœberprÃ¼fe die extrahierten EntitÃ¤ten auf PlausibilitÃ¤t.

SPEZIFISCHE CHECKS:
1. Material: Ist "Eiche" eine valide Holzart? (Check TIER 1)
2. Dimension: Ist "150 mÂ²" realistisch fÃ¼r einen Tisch? (PlausibilitÃ¤ts-Check)
3. OberflÃ¤che: Passt "geÃ¶lt" zu "Eiche"? (Domain Knowledge)

OUTPUT:
- verified: true/false (pro Entity)
- explanation: Warum akzeptiert/abgelehnt
- suggested_corrections: Falls Fehler erkannt

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AGENT_EXTRACT (Confidence 0.70-0.80):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AUFGABE:
Extrahiere alle relevanten Informationen NEU aus dem Originaltext.

ORIGINALTEXT (OCR):
{FULL_OCR_TEXT}

BISHERIGE EXTRAKTION (Unsicher):
{LOW_CONFIDENCE_ENTITIES}

OUTPUT:
- entities: VollstÃ¤ndige neue Extraktion
- comparison: Was unterscheidet sich zur OCR/NER-Extraktion?
- reasoning: Warum ist deine Extraktion besser?
```

### Memory Context Integration

```
Memory Retrieval Process:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. SHORT-TERM MEMORY (Redis)
   Query: Last 100 documents, same customer
   Purpose: Recent patterns, customer preferences
   Example: "Kunde Meier bestellt immer Buche statt Eiche"

2. LONG-TERM MEMORY (PostgreSQL)
   Query: Similar projects (K-NN, k=5)
   Similarity: Material + Dimension + Project Type
   Purpose: Historical pricing, common combinations
   Example: "Eichen-Tische 1-2 mÂ² meist mit Ã–l-Finish"

3. CONTEXT ASSEMBLY
   Limit: 10,000 tokens (avoid overwhelming LLM)
   Format: Structured JSON with most relevant projects
   Sorting: By similarity score (cosine distance)

Example Context JSON:
{
  "recent_by_customer": [
    {"project": "Buche-Tisch", "date": "2024-11-15", "material": "Buche"}
  ],
  "similar_projects": [
    {"project": "Eichen-Schreibtisch", "dim": "1.8 mÂ²", "surface": "geÃ¶lt"},
    ...
  ],
  "tier1_materials": ["Eiche", "Buche", "Kiefer", ...]
}
```

### Cost Tracking

```
Cost Calculation per Request:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Gemini 1.5 Flash Pricing (as of Dec 2024):
  Input:  $0.000075 per 1K tokens
  Output: $0.000300 per 1K tokens

Typical Request:
  Prompt:   ~1,500 tokens (system + context + task)
  Response: ~400 tokens (entities + explanation)

  Cost = (1.5 Ã— $0.000075) + (0.4 Ã— $0.000300)
       = $0.0001125 + $0.00012
       = $0.0002325 (~$0.00023)

Daily Budget: $10
Max Requests: $10 / $0.00023 â‰ˆ 43,000 requests/day

Actual Usage (Phase 3 stats):
  Average: 50-80 requests/day
  Cost: $0.012-0.018/day
  Budget headroom: 99.8%
```

### Error Handling

```
LLM Failure Scenarios:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. API TIMEOUT (> 10s)
   Fallback: Retry once with shorter context
   Ultimate: Route to HUMAN_REVIEW

2. INVALID JSON RESPONSE
   Action: Parse with error recovery (extract partial JSON)
   Fallback: Use OCR/NER result as-is

3. SAFETY FILTER TRIGGERED
   Cause: Document contains sensitive content
   Action: Log warning, route to HUMAN_REVIEW

4. QUOTA EXCEEDED
   Daily budget spent â†’ Disable Gemini for 24h
   All requests routed to HUMAN_REVIEW
   Email alert to admin

5. HALLUCINATION DETECTION
   Check: Are entities in TIER 1 data?
   If new material appears â†’ Flag for review
   Confidence penalty: -20%
```

---

## ğŸ”„ Pipeline Integration & Data Flow

### Full ML Pipeline Execution

```python
# Simplified code showing data flow
# File: extraction/services/integrated_pipeline.py

class IntegratedPipeline:
    def process_document(self, document_id: str) -> PipelineResult:
        doc = Document.objects.get(id=document_id)

        # STAGE 1: OCR
        ocr_service = GermanHandwerkOCRService()
        ocr_result = ocr_service.process_pdf(doc.file_path)
        # â†’ OCRResult(text, confidence=0.85, processing_time=2.3s)

        # STAGE 2: NER
        ner_service = GermanHandwerkNERService()
        entities = ner_service.extract_entities(ocr_result.full_text)
        # â†’ [Entity(text="Eiche", label="MATERIAL", conf=0.92), ...]

        # Calculate overall confidence
        overall_conf = (ocr_result.confidence * 0.4 +
                        avg(entities.confidence) * 0.6)
        # â†’ 0.88 (OCR 85% Ã— 0.4 + NER 90% Ã— 0.6)

        # STAGE 3: Confidence-based routing
        router = ConfidenceRouter()
        decision = router.route(overall_conf)
        # â†’ RoutingDecision(tier=AGENT_VERIFY, cost=0.00023)

        if decision.tier in [AGENT_VERIFY, AGENT_EXTRACT]:
            # Retrieve memory context
            memory_service = MemoryService()
            context = memory_service.retrieve_context(doc)
            # â†’ List of similar projects + TIER 1 data

            # Call Gemini
            gemini_service = GeminiAgentService()
            enhanced = gemini_service.enhance_extraction(
                entities=entities,
                context=context,
                task=decision.tier
            )
            # â†’ EnhancedEntities + explanation

            # Update entities with LLM results
            entities = enhanced.entities
            overall_conf = enhanced.confidence

        # Store results
        extraction_result = ExtractionResult.objects.create(
            document=doc,
            ocr_text=ocr_result.full_text,
            ocr_confidence=ocr_result.confidence,
            entities=entities,
            overall_confidence=overall_conf,
            routing_decision=decision.tier,
            gemini_enhanced=(decision.tier != AUTO_ACCEPT)
        )

        # Proceed to calculation if confidence high enough
        if overall_conf >= 0.80:
            calc_engine = CalculationEngine()
            price_result = calc_engine.calculate_price(entities)
            # â†’ CalculationResult(price=1268.29, breakdown={...})

        return PipelineResult(
            extraction=extraction_result,
            calculation=price_result
        )
```

### Performance Breakdown

```
Pipeline Stage Performance (Average):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Stage                Time      Bottleneck           Optimization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PDF â†’ Images         0.5s      IO + ImageMagick     Cache converted images
OCR (PaddleOCR)      2.0s      CPU (CNN inference)  Use GPU / batch processing
NER (spaCy)          0.3s      CPU (transformer)    Model quantization
Confidence Routing   0.05s     None                 -
Gemini API           2-4s      Network latency      Async batch requests
Calculation          0.1s      DB queries (TIER)    Cache TIER data
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL (no Gemini)    3.0s      OCR dominates
TOTAL (with Gemini)  5.5s      OCR + Gemini
```

### ML Model Updates & Versioning

```
Model Lifecycle:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PaddleOCR:
  Current: 2.7.0.3 (German models)
  Update frequency: 6-12 months (manual)
  Trigger: New PaddlePaddle release with German improvements
  Process: Test on validation set â†’ Compare accuracy â†’ Deploy

spaCy NER:
  Current: de_core_news_lg 3.8 + custom fine-tuning
  Update frequency: 3 months (custom) / 6 months (base model)
  Trigger:
    - Custom: Accumulated 500+ new training samples
    - Base: New spaCy major version
  Process:
    1. Collect corrections from HUMAN_REVIEW
    2. Re-annotate in IOB format
    3. Fine-tune NER component
    4. Validate on test set (F1 > 0.90 required)
    5. A/B test in staging (1 week)
    6. Deploy to production

Gemini:
  Current: gemini-1.5-flash (API, auto-updated by Google)
  Update frequency: Continuous (Google manages)
  Version control: None (API abstraction)
  Monitoring: Track response quality, cost per request
```

---

## ğŸ“Š ML Evaluation & Monitoring

### Key Metrics

```
Metrics Dashboard (Real-time):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OCR Metrics:
  - Avg Confidence: 0.87 (Target: >0.85)
  - Processing Time: 2.1s/page (Target: <3s)
  - Error Rate: 1.2% (blank results, crashes)

NER Metrics:
  - Entity Recall: 0.93 (found 93% of entities)
  - Entity Precision: 0.95 (95% of found entities correct)
  - F1 Score: 0.94 (harmonic mean)
  - Confidence Distribution:
      0.9-1.0: 78% (high confidence)
      0.8-0.9: 15% (medium)
      <0.8:    7%  (low, routed to Gemini)

Gemini Metrics:
  - Usage Rate: 12% of documents (0.80-0.92 confidence)
  - Improvement: +8% average confidence boost
  - Cost per Enhanced Doc: $0.00023
  - Daily Cost: $0.015 (Budget: $10)

End-to-End Metrics:
  - Documents without correction: 89% (Target: >85%)
  - Avg processing time: 3.2s (Target: <5s)
  - User satisfaction: 4.7/5.0
```

### A/B Testing Framework

```
Testing New Models:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: New spaCy NER model with improved SURFACE detection

1. BASELINE (7 days)
   Model: Current production model
   Traffic: 100%
   Collect: Entity accuracy, confidence scores

2. CANARY DEPLOYMENT (7 days)
   Model A (control): Current (90% traffic)
   Model B (test): New model (10% traffic)
   Compare: F1 score, processing time, user corrections

3. RAMP-UP (7 days)
   If Model B better: 50/50 split
   Monitor: No regressions in other entity types

4. FULL ROLLOUT
   If all metrics improved: 100% Model B
   Rollback trigger: F1 drops >2% OR processing time >1.5Ã—
```

---

**NÃ¤chste Datei:** `KNOWLEDGE_INTEGRATION.md` - Detaillierte ErklÃ¤rung des TIER 1/2/3 Systems.
